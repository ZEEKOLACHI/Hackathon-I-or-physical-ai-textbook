---
id: ch-2-04
title: Computer Vision for Robotics
sidebar_position: 1
difficulty: intermediate
estimated_time: 90
prerequisites: [ch-1-03]
---

# Computer Vision for Robotics: Teaching Machines to See

> *"The question of whether machines can think is about as relevant as the question of whether submarines can swim."*
> â€” Edsger Dijkstra

When a human infant opens their eyes for the first time, they begin a journey of visual learning that will take years to mature. A newborn cannot distinguish faces, estimate distances, or recognize objectsâ€”skills we take for granted as adults. Teaching robots to see presents an even greater challenge: we must explicitly encode the visual understanding that humans acquire through billions of neural connections refined over millennia of evolution.

## The Philosophy of Machine Perception

### What Does It Mean to "See"?

Vision is not merely the capture of photons. It is the construction of meaning from patterns of light. When you look at a coffee mug, you don't perceive a collection of pixelsâ€”you perceive an object with affordances: it can be grasped, filled, lifted, and drunk from. This leap from sensation to understanding represents the fundamental challenge of computer vision.

```
The Vision Pipeline: From Photons to Understanding
=================================================

    PHYSICAL         SENSOR          COMPUTATIONAL        COGNITIVE
    WORLD            CAPTURE         PROCESSING           UNDERSTANDING
       â”‚                â”‚                 â”‚                    â”‚
       â–¼                â–¼                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Light     â”‚  â”‚   Camera    â”‚  â”‚   Image     â”‚    â”‚  Scene          â”‚
â”‚   reflects  â”‚â†’ â”‚   sensor    â”‚â†’ â”‚   processingâ”‚ â†’  â”‚  understanding  â”‚
â”‚   off       â”‚  â”‚   captures  â”‚  â”‚   extracts  â”‚    â”‚  enables        â”‚
â”‚   surfaces  â”‚  â”‚   photons   â”‚  â”‚   features  â”‚    â”‚  action         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Physical         ~1-10ms         ~10-100ms          ~100-1000ms
    phenomena        (sensor         (traditional)       (deep learning)
                     latency)
```

**Three Levels of Visual Understanding:**

| Level | Description | Robot Capability | Example |
|-------|-------------|------------------|---------|
| **Detection** | "Something is there" | Obstacle avoidance | LiDAR detects object in path |
| **Recognition** | "It's a chair" | Object manipulation | Identify graspable items |
| **Understanding** | "Someone wants to sit" | Social interaction | Offer the chair to a person |

### Historical Context: The Long Road to Machine Vision

The history of computer vision is a story of humbling realizations. In 1966, MIT professor Seymour Papert assigned "solving vision" as a summer project for undergraduate students. Sixty years later, we're still working on it.

**Key Milestones in Computer Vision History:**

| Year | Milestone | Significance |
|------|-----------|--------------|
| 1963 | Larry Roberts' "Blocks World" | First 3D object recognition from 2D images |
| 1970 | Marr's computational theory | Framework for understanding vision as computation |
| 1980 | Canny edge detector | Still-used technique for edge detection |
| 1999 | SIFT features | Scale-invariant object recognition |
| 2012 | AlexNet | Deep learning revolution begins |
| 2015 | ResNet | Superhuman image classification |
| 2020 | Vision Transformers | Attention mechanisms transform vision |
| 2023 | Foundation models | Zero-shot understanding emerges |

> *"In the 1960s, we thought vision was easy and language was hard. We had it exactly backwards."*
> â€” Takeo Kanade, Robotics Pioneer

## The Camera: Understanding Your Robot's Eyes

### The Pinhole Camera Model: Geometry of Projection

Every camera, from a smartphone to a industrial machine vision system, approximates the behavior of a pinhole cameraâ€”a box with a tiny aperture through which light enters and projects onto a surface.

```
The Pinhole Camera Geometry
===========================

                           3D World Point
                                P(X,Y,Z)
                                   *
                                  /â”‚
                                 / â”‚
                                /  â”‚
                               /   â”‚
                              /    â”‚
                        Z    /     â”‚
                        â”‚   /      â”‚
                        â”‚  /       â”‚
           Image Plane  â”‚ /        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€
              â”‚         â”‚(u,v)     â”‚
              â”‚         *â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â†’ X
              â”‚        /â”‚          â”‚
              â”‚       / â”‚          â”‚
              â”‚      /  â”‚          â”‚
              â”‚     /   â”‚
              â”‚    /    â”‚
              â”‚   /  f  â”‚ (focal length)
              â”‚  /      â”‚
              â””â”€/â”€â”€â”€â”€â”€â”€â”€â”˜
               /
              â—‹ Camera Center (Optical Center)
               \
                \
                 â†’ Y

    Projection equations:
    u = f Ã— (X/Z) + cx
    v = f Ã— (Y/Z) + cy

    Where (cx, cy) is the principal point (image center)
```

**The Intrinsic Matrix:**

The camera's intrinsic parameters describe its internal geometry, encoded in a 3x3 matrix:

```
        â”Œ                      â”
        â”‚  fx    0    cx       â”‚
    K = â”‚  0     fy   cy       â”‚
        â”‚  0     0    1        â”‚
        â””                      â”˜

    fx, fy : Focal lengths in pixels (may differ for non-square pixels)
    cx, cy : Principal point coordinates (image center offset)
```

**Why Calibration Matters:**

An uncalibrated camera is like a ruler with unmarked unitsâ€”you can see relative differences but cannot measure actual distances. For a robot attempting to grasp an object 30cm away, a 10% calibration error means the gripper arrives 3cm off target.

| Calibration Error | Effect on 1m Distance | Practical Impact |
|-------------------|----------------------|------------------|
| 1% | 1 cm error | Minor positioning issues |
| 5% | 5 cm error | Grasp failures common |
| 10% | 10 cm error | Navigation unreliable |
| 20% | 20 cm error | System unusable |

### Lens Distortion: When Straight Lines Curve

Real lenses are not perfect. They introduce distortions that warp the image, most notably:

```
Types of Lens Distortion
========================

    Barrel Distortion         Pincushion Distortion      Ideal (No Distortion)
    (wide-angle lenses)       (telephoto lenses)

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®    â”‚       â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®    â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚ â•­â”‚         â”‚â•®   â”‚       â”‚ â•¯â”‚         â”‚â•°   â”‚        â”‚  â”‚         â”‚    â”‚
    â”‚ â”‚           â”‚   â”‚       â”‚  â”‚         â”‚    â”‚        â”‚  â”‚         â”‚    â”‚
    â”‚ â”‚           â”‚   â”‚       â”‚  â”‚         â”‚    â”‚        â”‚  â”‚         â”‚    â”‚
    â”‚ â•°â”‚         â”‚â•¯   â”‚       â”‚ â•®â”‚         â”‚â•­   â”‚        â”‚  â”‚         â”‚    â”‚
    â”‚  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯    â”‚       â”‚  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯    â”‚        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Lines bow outward          Lines bow inward           Lines remain straight
    Common in GoPro,           Common in zoom            After calibration
    smartphone cameras         lenses
```

Distortion correction is essential for:
- Accurate 3D reconstruction
- Visual odometry (camera-based motion estimation)
- Object measurement
- Multi-camera systems

### Stereo Vision: Depth from Geometry

Humans perceive depth through binocular visionâ€”our two eyes provide slightly different views that the brain combines into 3D understanding. Stereo cameras replicate this principle.

```
Stereo Vision Geometry
=====================

                    P (3D point)
                       *
                      /|\
                     / | \
                    /  |  \
                   /   |   \
                  /    |    \
                 /     |d    \    d = depth (what we want to find)
                /      |      \
               /       |       \
    Left     /    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”   \  Right
    Camera  â—‹â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€*â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â—‹ Camera
               pl â”‚         â”‚ pr
                  â”‚    b    â”‚      b = baseline (known)
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Disparity: Î´ = pl - pr (pixel difference between left and right images)

    Depth equation: d = (f Ã— b) / Î´

    Where:
    - f = focal length
    - b = baseline (distance between cameras)
    - Î´ = disparity (must be > 0)
```

**The Stereo Matching Challenge:**

Finding corresponding points between left and right images (stereo matching) is computationally demanding. The fundamental question: which pixel in the left image corresponds to which pixel in the right?

| Matching Method | Speed | Accuracy | Best For |
|-----------------|-------|----------|----------|
| Block matching | Fast | Low | Real-time, textured scenes |
| Semi-global matching | Medium | High | General purpose |
| Deep stereo | Slow | Very High | Offline processing |
| Learned features | Medium | High | Challenging conditions |

## Image Processing Fundamentals: The Building Blocks

### The Image as Data Structure

A digital image is, fundamentally, a matrix of numbers. Understanding this representation is essential for all subsequent processing.

```
Image Data Representation
========================

    Grayscale Image (H Ã— W):              Color Image (H Ã— W Ã— 3):

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 128  135  142  148  â”‚               â”‚  R    G    B        â”‚
    â”‚ 141  156  163  172  â”‚               â”‚ â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”       â”‚
    â”‚ 153  169  184  195  â”‚               â”‚ â”‚128â”‚ 45â”‚ 12â”‚       â”‚
    â”‚ 162  178  196  212  â”‚               â”‚ â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚ â”‚141â”‚ 52â”‚ 18â”‚       â”‚
                                          â”‚ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜       â”‚
    Each value: 0-255 (8-bit)             â”‚    per pixel        â”‚
    0 = black, 255 = white                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Memory Layout:
    - 640Ã—480 grayscale = 307,200 bytes (300 KB)
    - 640Ã—480 RGB = 921,600 bytes (900 KB)
    - 1920Ã—1080 RGB = 6,220,800 bytes (6 MB)
```

### Convolution: The Universal Image Operation

Nearly every image processing operation can be expressed as a convolutionâ€”sliding a small "kernel" across the image and computing weighted sums at each position.

```
Convolution Operation
====================

    Input Image          Kernel (3Ã—3)         Output
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ a b c d e f   â”‚    â”‚ w1 w2 w3â”‚    Apply kernel at each position:
    â”‚ g h i j k l   â”‚  Ã— â”‚ w4 w5 w6â”‚  = Sum of element-wise products
    â”‚ m n o p q r   â”‚    â”‚ w7 w8 w9â”‚
    â”‚ s t u v w x   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Example: Computing output at position (1,1):

    output[1,1] = aÃ—w1 + bÃ—w2 + cÃ—w3 +
                  gÃ—w4 + hÃ—w5 + iÃ—w6 +
                  mÃ—w7 + nÃ—w8 + oÃ—w9
```

**Common Kernels and Their Effects:**

| Kernel Name | Effect | Kernel Values | Use in Robotics |
|-------------|--------|---------------|-----------------|
| **Identity** | No change | [0,0,0; 0,1,0; 0,0,0] | Baseline |
| **Gaussian Blur** | Smoothing | Gaussian distribution | Noise reduction |
| **Sobel X** | Vertical edges | [-1,0,1; -2,0,2; -1,0,1] | Edge detection |
| **Sobel Y** | Horizontal edges | [1,2,1; 0,0,0; -1,-2,-1] | Edge detection |
| **Laplacian** | All edges | [0,1,0; 1,-4,1; 0,1,0] | Feature detection |
| **Sharpen** | Enhance edges | [0,-1,0; -1,5,-1; 0,-1,0] | Detail enhancement |

### Edge Detection: Finding Boundaries

Edgesâ€”abrupt changes in intensityâ€”often correspond to object boundaries, making them crucial for robot perception.

```
Edge Detection Pipeline
======================

    Original          Smoothed           Gradients         Edges
    (with noise)      (noise reduced)    (derivatives)     (thresholded)

    â–‘â–’â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘      â–‘â–‘â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘       Â·Â·Â·Â·Â·â–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·       Â·Â·Â·Â·Â·â”Œâ”€â”€â”€â”€Â·
    â–‘â–‘â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’      â–‘â–‘â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘       Â·Â·Â·Â·Â·â–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·       Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·
    â–‘â–’â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘   â†’  â–‘â–‘â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘   â†’   Â·Â·Â·Â·Â·â–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·   â†’   Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·
    â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘      â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘        Â·Â·Â·Â·Â·â–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·       Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·
    â–‘â–‘â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–’      â–‘â–‘â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘       Â·Â·Â·Â·Â·â–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·       Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·

       Noise           Gaussian           Sobel/           Hysteresis
       present          blur              Canny            thresholding
```

**The Canny Edge Detector:**

John Canny's 1986 algorithm remains the gold standard for edge detection, optimizing three criteria:
1. **Good detection**: Find all real edges
2. **Good localization**: Edges should be close to true positions
3. **Single response**: One edge should not produce multiple detections

### Feature Detection: Finding Distinctive Points

Features are distinctive, repeatable points that can be reliably detected across different images of the same sceneâ€”essential for visual odometry and object recognition.

```
Feature Detection Concepts
=========================

    What Makes a Good Feature?

    CORNER (Good)         EDGE (Poor)          FLAT (Poor)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚             â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚            â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚             â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚            â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ”‚             â”‚â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚            â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ”‚             â”‚â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚            â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ”‚             â”‚â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚            â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”˜

    Unique in all        Ambiguous along       Ambiguous in
    directions           the edge              all directions
```

**Evolution of Feature Detectors:**

| Detector | Year | Key Innovation | Speed | Robustness |
|----------|------|----------------|-------|------------|
| Harris Corner | 1988 | Eigenvalue analysis | Fast | Moderate |
| SIFT | 1999 | Scale invariance | Slow | Excellent |
| SURF | 2006 | Integral images | Medium | Good |
| ORB | 2011 | Binary descriptors | Very Fast | Good |
| SuperPoint | 2018 | Deep learning | Fast (GPU) | Excellent |

## Object Detection: From Pixels to Semantics

### The Evolution of Object Detection

The journey from simple template matching to modern neural networks represents one of AI's greatest success stories.

**Three Paradigms of Object Detection:**

```
Historical Evolution of Object Detection
========================================

    ERA 1: Hand-crafted (2001-2012)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Image â†’ HOG Features â†’ SVM Classifier â”‚
    â”‚                                        â”‚
    â”‚  + Interpretable, fast                 â”‚
    â”‚  - Limited accuracy, manual design     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    ERA 2: Two-stage CNNs (2012-2017)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Image â†’ Region Proposals â†’ CNN â†’      â”‚
    â”‚          Classification                â”‚
    â”‚                                        â”‚
    â”‚  + High accuracy                       â”‚
    â”‚  - Slow (R-CNN: 47s per image!)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    ERA 3: Single-shot Detectors (2016-present)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Image â†’ CNN â†’ Boxes + Classes (once)  â”‚
    â”‚                                        â”‚
    â”‚  + Real-time (YOLO: 45 FPS)           â”‚
    â”‚  + Good accuracy                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Understanding Modern Architectures

**YOLO (You Only Look Once):**

YOLO revolutionized object detection by framing it as a single regression problemâ€”predict bounding boxes and class probabilities directly from full images in one evaluation.

```
YOLO Detection Principle
========================

    Input Image                 Grid Division              Predictions
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       For each cell:
    â”‚                 â”‚        â”‚     â”‚     â”‚     â”‚       - B bounding boxes
    â”‚    ğŸš—  ğŸš¶       â”‚   â†’    â”‚â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”‚   â†’   - Confidence scores
    â”‚                 â”‚        â”‚     â”‚     â”‚     â”‚       - Class probabilities
    â”‚        ğŸš™       â”‚        â”‚â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”‚
    â”‚                 â”‚        â”‚     â”‚     â”‚     â”‚       Total predictions:
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       S Ã— S Ã— (BÃ—5 + C)

    S = grid size (e.g., 7)
    B = boxes per cell (e.g., 2)
    C = number of classes (e.g., 20)
```

**Comparison of Detection Architectures:**

| Architecture | Speed (FPS) | mAP | Best For |
|--------------|-------------|-----|----------|
| Faster R-CNN | 5-7 | High | Accuracy-critical |
| SSD | 45 | Medium | Balance |
| YOLOv5 | 140 | High | Real-time robotics |
| YOLOv8 | 150+ | Very High | State-of-the-art |
| DETR | 28 | High | No anchor boxes |

### Semantic Segmentation: Pixel-Level Understanding

While detection draws boxes around objects, segmentation classifies every pixelâ€”essential for robots navigating complex environments.

```
Detection vs. Segmentation
==========================

    Object Detection:               Semantic Segmentation:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
    â”‚   â”‚  CAR    â”‚           â”‚    â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â”‚   â”‚         â”‚           â”‚    â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚
    â”‚                         â”‚    â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    "There's a car here"           "Sky, car, roadâ€”every pixel"
    Bounding box only              Full scene understanding

    Legend: â–ˆ = sky, â–‘ = car, â–“ = road
```

## Depth Perception: Adding the Third Dimension

### Methods for Obtaining Depth

Robots need depth information to interact with the 3D world. Several technologies provide this:

```
Depth Sensing Technologies
=========================

    Technology          Range        Accuracy    Environment    Cost
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Stereo Vision       1-20m        ~1%         Indoor/Outdoor $$
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â—‹   â—‹ â”‚ Two cameras, triangulation
    â””â”€â”€â”€â”€â”€â”€â”€â”˜

    Structured Light    0.3-4m       ~0.1%       Indoor only    $$
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â‰‹  â—‰ â”‚ Pattern projector + camera
    â””â”€â”€â”€â”€â”€â”€â”€â”˜

    Time of Flight      0.1-10m      ~1%         Indoor/Outdoor $$$
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚ )))â—‰ â”‚ Measures light travel time
    â””â”€â”€â”€â”€â”€â”€â”€â”˜

    LiDAR               1-200m       ~cm         All conditions $$$$
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â•±â•²â•±â•² â”‚ Laser scanning
    â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Point Clouds: The 3D Data Structure

Depth sensors produce point cloudsâ€”collections of 3D points representing surfaces in the environment.

```
Point Cloud Representation
=========================

    2D Image View:              3D Point Cloud:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    * *
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚               * *     * *
    â”‚   â”‚  Chair  â”‚   â”‚            *     â”Œâ”€â”€â”€â”€â”€â”  *
    â”‚   â”‚         â”‚   â”‚           *      â”‚     â”‚   *
    â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚          *       â”‚     â”‚    *
    â”‚       â•â•§â•       â”‚         *        â””â”€â”€â”¬â”€â”€â”˜     *
    â”‚                 â”‚        *            â”‚         *
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       * * * * * *â•â•â•â•§â•â•â•* * * * *

    Pixels: (u, v, color)      Points: (x, y, z, [color])
    Dense, regular grid        Sparse, irregular distribution
```

## Vision for Navigation: Visual Odometry and SLAM

### Visual Odometry: Motion from Vision

Visual odometry (VO) estimates camera motion by tracking features across sequential imagesâ€”enabling robots to navigate without GPS.

```
Visual Odometry Pipeline
========================

    Frame t                Frame t+1              Motion Estimate
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   * *       â”‚        â”‚     * *     â”‚        Î”x = 0.15m
    â”‚  *   *      â”‚   â†’    â”‚    *   *    â”‚   â†’    Î”y = 0.02m
    â”‚      *  *   â”‚        â”‚        *  * â”‚        Î”Î¸ = 5Â°
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  Accumulated:
    Features detected      Features tracked       Position estimate
    and matched            Motion computed        over time
```

### SLAM: Simultaneous Localization and Mapping

SLAM solves a chicken-and-egg problem: to know where you are, you need a map; to build a map, you need to know where you are.

```
The SLAM Problem
===============

    Chicken-and-Egg Dilemma:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ To localize:    â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚ To map:         â”‚
    â”‚ Need a map      â”‚         â”‚ Need position   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚           â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ SLAM: Do both   â”‚
                â”‚ simultaneously! â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Types of Visual SLAM:**

| Method | Key Idea | Pros | Cons |
|--------|----------|------|------|
| **Feature-based** (ORB-SLAM) | Track sparse features | Efficient, robust | Needs texture |
| **Direct** (LSD-SLAM) | Use all pixels | Dense maps | Computationally heavy |
| **Deep** (DROID-SLAM) | Learned features | State-of-the-art | Requires GPU |

## Practical Considerations for Robotic Vision

### Real-Time Performance

Vision systems must keep up with robot motion. A humanoid running at 3 m/s covers 10 cm per frame at 30 FPSâ€”that's significant for obstacle avoidance.

**Latency Budget for Robot Vision:**

```
Vision Pipeline Timing
=====================

    Component               Typical Time    Target Time
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Image capture           1-10 ms         < 5 ms
    Preprocessing           2-5 ms          < 3 ms
    Feature extraction      5-20 ms         < 10 ms
    Object detection        10-50 ms        < 30 ms
    Post-processing         2-5 ms          < 3 ms
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    TOTAL                   20-90 ms        < 50 ms

    Target: < 50ms for 20 Hz operation
```

### Robustness Challenges

Real-world conditions challenge vision systems in ways lab demonstrations never reveal:

| Challenge | Effect | Mitigation |
|-----------|--------|------------|
| Motion blur | Feature tracking fails | Higher frame rate, prediction |
| Varying lighting | Exposure issues, shadows | HDR, adaptive algorithms |
| Reflections | False features, confusion | Polarization filters, learning |
| Occlusion | Objects partially hidden | Multi-view, prediction |
| Weather | Rain, fog, snow degrade visibility | Sensor fusion, radar backup |

## Summary: The Visual Foundation of Robotic Intelligence

Computer vision transforms robots from blind actuators into perceiving agents capable of understanding and interacting with their environment. The journey from pixels to understanding encompasses:

**Key Takeaways:**

1. **Camera geometry matters**: Proper calibration is the foundation of accurate 3D perception. A miscalibrated camera renders all downstream processing unreliable.

2. **The feature hierarchy**: From edges to features to objects to scenesâ€”each level builds on the previous, adding semantic richness.

3. **Deep learning has transformed detection**: Neural networks achieve superhuman performance on many vision tasks, but require careful training data and significant computation.

4. **Depth perception enables interaction**: 2D images alone cannot support manipulation; depth sensing technologies provide the missing dimension.

5. **Real-time constraints shape solutions**: The best algorithm that's too slow is worse than a good algorithm that runs in time.

The perception capabilities covered in this chapter form the sensory foundation for everything that followsâ€”planning, manipulation, and navigation all depend on accurate visual understanding of the world.

---

## Further Reading

**Foundational Texts:**
- Szeliski, R. "Computer Vision: Algorithms and Applications" (2022 edition) - Comprehensive reference
- Hartley & Zisserman, "Multiple View Geometry" - The Bible of geometric vision
- Goodfellow, Bengio & Courville, "Deep Learning" - Neural network foundations

**Key Papers:**
- Lowe, D. "Distinctive Image Features from Scale-Invariant Keypoints" (SIFT)
- Redmon et al., "You Only Look Once" (YOLO)
- Mur-Artal et al., "ORB-SLAM: A Versatile and Accurate Monocular SLAM System"

**Online Resources:**
- [OpenCV Documentation](https://docs.opencv.org)
- [PyTorch Vision Library](https://pytorch.org/vision)
- [Papers With Code - Object Detection](https://paperswithcode.com/task/object-detection)
